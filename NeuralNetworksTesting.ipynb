{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworksTesting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eSEHc7otENBL"
      },
      "outputs": [],
      "source": [
        "from numpy import loadtxt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRHKoFWJEPBF",
        "outputId": "419162cc-ebdb-493e-babb-ccec28aab49f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from gensim.models import word2vec, FastText\n",
        "import re\n",
        "from sklearn.metrics import ( accuracy_score, roc_auc_score,\n",
        "                             precision_score, recall_score, f1_score,fbeta_score,\n",
        "                             classification_report\n",
        "                             )\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as shc\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn import cluster, datasets, mixture\n",
        "from sklearn import metrics\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "code = pd.read_csv(\"/content/drive/MyDrive/EvaluationTSE/code2vec/formData.txt\", sep=',', header=None)\n",
        "code = code[code.columns[:-1]]\n",
        "code.head(5)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "code = shuffle(code)\n",
        "print(code.head(5))\n",
        "\n",
        "code = code.values\n",
        "# X = code.iloc[:,:].values\n",
        "\n",
        "# z=pd.DataFrame(X)\n",
        "# z\n",
        "\n",
        "X= code[:, 1:-1]\n",
        "y= code[:,-1]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Se2aR0EUAq",
        "outputId": "e0c252bf-1abd-43fe-eb48-4c36f7fccd41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0         1         2         3         4         5         6    \\\n",
            "6636 -0.319887 -0.965951  0.680421  0.454728 -0.885136  0.184122 -0.227379   \n",
            "2420 -0.651814  0.461307 -0.111965  0.949824 -0.585111  0.282018  0.459835   \n",
            "9712  0.937618  0.233876  0.468538  0.161834 -0.479426 -0.355915 -0.874760   \n",
            "1470  0.375283 -0.092723  0.707655  0.169764  0.069865 -0.202253 -0.618900   \n",
            "665  -0.028148  0.435183  0.877439  0.531538 -0.260106  0.369770 -0.323203   \n",
            "\n",
            "           7         8         9    ...       375       376       377  \\\n",
            "6636 -0.447671 -0.800908  0.556983  ...  0.837419  0.533305  0.424085   \n",
            "2420  0.009923 -0.480117  0.664029  ... -0.485715  0.306358 -0.350461   \n",
            "9712  0.416933  0.423980 -0.386938  ... -0.738832  0.972285 -0.396771   \n",
            "1470 -0.035899  0.534599  0.349887  ... -0.135286 -0.395025  0.305235   \n",
            "665  -0.317040 -0.772302  0.891540  ...  0.783629  0.824974 -0.105631   \n",
            "\n",
            "           378       379       380       381       382       383  384  \n",
            "6636 -0.621319  0.822149  0.569612  0.767692  0.082346 -0.707799    1  \n",
            "2420  0.717222  0.705781 -0.121317  0.453019  0.734096  0.256391    0  \n",
            "9712 -0.415108 -0.369128  0.254128  0.733501 -0.391331 -0.229528    1  \n",
            "1470 -0.213599  0.271932 -0.123991 -0.154001  0.390077  0.432832    0  \n",
            "665  -0.646177  0.972495  0.874803 -0.120109  0.135036  0.472999    0  \n",
            "\n",
            "[5 rows x 385 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-lysvEvNF0b",
        "outputId": "0fd9c319-e23c-4f72-ea8d-e7b4c533c0de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.18816948 -0.10361806 -0.5454442  ...  0.5040214   0.36716452\n",
            "  -0.46575624]\n",
            " [-0.17064357 -0.36324653 -0.01171706 ...  0.24676433  0.34938282\n",
            "  -0.7278178 ]\n",
            " [ 0.44743764  0.6439825  -0.10347992 ... -0.82652676 -0.1768982\n",
            "   0.00332761]\n",
            " ...\n",
            " [-0.61213905 -0.03117782  0.15601894 ...  0.13216257 -0.33673042\n",
            "   0.36352426]\n",
            " [-0.00542263  0.56690633 -0.09812792 ...  0.83336234 -0.17673723\n",
            "  -0.00534876]\n",
            " [ 0.08417466 -0.43425208  0.3318141  ... -0.62645984 -0.5164243\n",
            "  -0.70976037]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = X.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n"
      ],
      "metadata": {
        "id": "XfZrPual933s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUrfu0_cAkM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = code.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "# compile the keras model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "error = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSuenWQzNLYG",
        "outputId": "280b80af-98e5-4716-c53f-bef86bb58d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "216/216 - 1s - loss: 0.3609 - 853ms/epoch - 4ms/step\n",
            "Epoch 2/150\n",
            "216/216 - 0s - loss: 0.2721 - 273ms/epoch - 1ms/step\n",
            "Epoch 3/150\n",
            "216/216 - 0s - loss: 0.2587 - 274ms/epoch - 1ms/step\n",
            "Epoch 4/150\n",
            "216/216 - 0s - loss: 0.2517 - 273ms/epoch - 1ms/step\n",
            "Epoch 5/150\n",
            "216/216 - 0s - loss: 0.2480 - 273ms/epoch - 1ms/step\n",
            "Epoch 6/150\n",
            "216/216 - 0s - loss: 0.2449 - 268ms/epoch - 1ms/step\n",
            "Epoch 7/150\n",
            "216/216 - 0s - loss: 0.2425 - 277ms/epoch - 1ms/step\n",
            "Epoch 8/150\n",
            "216/216 - 0s - loss: 0.2403 - 269ms/epoch - 1ms/step\n",
            "Epoch 9/150\n",
            "216/216 - 0s - loss: 0.2386 - 266ms/epoch - 1ms/step\n",
            "Epoch 10/150\n",
            "216/216 - 0s - loss: 0.2373 - 262ms/epoch - 1ms/step\n",
            "Epoch 11/150\n",
            "216/216 - 0s - loss: 0.2360 - 317ms/epoch - 1ms/step\n",
            "Epoch 12/150\n",
            "216/216 - 0s - loss: 0.2348 - 263ms/epoch - 1ms/step\n",
            "Epoch 13/150\n",
            "216/216 - 0s - loss: 0.2344 - 274ms/epoch - 1ms/step\n",
            "Epoch 14/150\n",
            "216/216 - 0s - loss: 0.2328 - 282ms/epoch - 1ms/step\n",
            "Epoch 15/150\n",
            "216/216 - 0s - loss: 0.2325 - 258ms/epoch - 1ms/step\n",
            "Epoch 16/150\n",
            "216/216 - 0s - loss: 0.2318 - 265ms/epoch - 1ms/step\n",
            "Epoch 17/150\n",
            "216/216 - 0s - loss: 0.2312 - 274ms/epoch - 1ms/step\n",
            "Epoch 18/150\n",
            "216/216 - 0s - loss: 0.2299 - 272ms/epoch - 1ms/step\n",
            "Epoch 19/150\n",
            "216/216 - 0s - loss: 0.2297 - 259ms/epoch - 1ms/step\n",
            "Epoch 20/150\n",
            "216/216 - 0s - loss: 0.2293 - 261ms/epoch - 1ms/step\n",
            "Epoch 21/150\n",
            "216/216 - 0s - loss: 0.2283 - 280ms/epoch - 1ms/step\n",
            "Epoch 22/150\n",
            "216/216 - 0s - loss: 0.2263 - 301ms/epoch - 1ms/step\n",
            "Epoch 23/150\n",
            "216/216 - 0s - loss: 0.2259 - 257ms/epoch - 1ms/step\n",
            "Epoch 24/150\n",
            "216/216 - 0s - loss: 0.2259 - 268ms/epoch - 1ms/step\n",
            "Epoch 25/150\n",
            "216/216 - 0s - loss: 0.2252 - 279ms/epoch - 1ms/step\n",
            "Epoch 26/150\n",
            "216/216 - 0s - loss: 0.2249 - 266ms/epoch - 1ms/step\n",
            "Epoch 27/150\n",
            "216/216 - 0s - loss: 0.2234 - 275ms/epoch - 1ms/step\n",
            "Epoch 28/150\n",
            "216/216 - 0s - loss: 0.2229 - 266ms/epoch - 1ms/step\n",
            "Epoch 29/150\n",
            "216/216 - 0s - loss: 0.2223 - 274ms/epoch - 1ms/step\n",
            "Epoch 30/150\n",
            "216/216 - 0s - loss: 0.2215 - 268ms/epoch - 1ms/step\n",
            "Epoch 31/150\n",
            "216/216 - 0s - loss: 0.2212 - 268ms/epoch - 1ms/step\n",
            "Epoch 32/150\n",
            "216/216 - 0s - loss: 0.2213 - 277ms/epoch - 1ms/step\n",
            "Epoch 33/150\n",
            "216/216 - 0s - loss: 0.2209 - 270ms/epoch - 1ms/step\n",
            "Epoch 34/150\n",
            "216/216 - 0s - loss: 0.2204 - 259ms/epoch - 1ms/step\n",
            "Epoch 35/150\n",
            "216/216 - 0s - loss: 0.2190 - 273ms/epoch - 1ms/step\n",
            "Epoch 36/150\n",
            "216/216 - 0s - loss: 0.2189 - 265ms/epoch - 1ms/step\n",
            "Epoch 37/150\n",
            "216/216 - 0s - loss: 0.2182 - 276ms/epoch - 1ms/step\n",
            "Epoch 38/150\n",
            "216/216 - 0s - loss: 0.2183 - 264ms/epoch - 1ms/step\n",
            "Epoch 39/150\n",
            "216/216 - 0s - loss: 0.2172 - 255ms/epoch - 1ms/step\n",
            "Epoch 40/150\n",
            "216/216 - 0s - loss: 0.2175 - 287ms/epoch - 1ms/step\n",
            "Epoch 41/150\n",
            "216/216 - 0s - loss: 0.2167 - 304ms/epoch - 1ms/step\n",
            "Epoch 42/150\n",
            "216/216 - 0s - loss: 0.2165 - 277ms/epoch - 1ms/step\n",
            "Epoch 43/150\n",
            "216/216 - 0s - loss: 0.2155 - 263ms/epoch - 1ms/step\n",
            "Epoch 44/150\n",
            "216/216 - 0s - loss: 0.2152 - 268ms/epoch - 1ms/step\n",
            "Epoch 45/150\n",
            "216/216 - 0s - loss: 0.2155 - 253ms/epoch - 1ms/step\n",
            "Epoch 46/150\n",
            "216/216 - 0s - loss: 0.2148 - 270ms/epoch - 1ms/step\n",
            "Epoch 47/150\n",
            "216/216 - 0s - loss: 0.2145 - 258ms/epoch - 1ms/step\n",
            "Epoch 48/150\n",
            "216/216 - 0s - loss: 0.2141 - 295ms/epoch - 1ms/step\n",
            "Epoch 49/150\n",
            "216/216 - 0s - loss: 0.2136 - 266ms/epoch - 1ms/step\n",
            "Epoch 50/150\n",
            "216/216 - 0s - loss: 0.2137 - 261ms/epoch - 1ms/step\n",
            "Epoch 51/150\n",
            "216/216 - 0s - loss: 0.2135 - 275ms/epoch - 1ms/step\n",
            "Epoch 52/150\n",
            "216/216 - 0s - loss: 0.2134 - 282ms/epoch - 1ms/step\n",
            "Epoch 53/150\n",
            "216/216 - 0s - loss: 0.2128 - 296ms/epoch - 1ms/step\n",
            "Epoch 54/150\n",
            "216/216 - 0s - loss: 0.2123 - 268ms/epoch - 1ms/step\n",
            "Epoch 55/150\n",
            "216/216 - 0s - loss: 0.2123 - 271ms/epoch - 1ms/step\n",
            "Epoch 56/150\n",
            "216/216 - 0s - loss: 0.2117 - 261ms/epoch - 1ms/step\n",
            "Epoch 57/150\n",
            "216/216 - 0s - loss: 0.2119 - 260ms/epoch - 1ms/step\n",
            "Epoch 58/150\n",
            "216/216 - 0s - loss: 0.2119 - 261ms/epoch - 1ms/step\n",
            "Epoch 59/150\n",
            "216/216 - 0s - loss: 0.2116 - 265ms/epoch - 1ms/step\n",
            "Epoch 60/150\n",
            "216/216 - 0s - loss: 0.2110 - 263ms/epoch - 1ms/step\n",
            "Epoch 61/150\n",
            "216/216 - 0s - loss: 0.2109 - 273ms/epoch - 1ms/step\n",
            "Epoch 62/150\n",
            "216/216 - 0s - loss: 0.2101 - 258ms/epoch - 1ms/step\n",
            "Epoch 63/150\n",
            "216/216 - 0s - loss: 0.2110 - 277ms/epoch - 1ms/step\n",
            "Epoch 64/150\n",
            "216/216 - 0s - loss: 0.2105 - 269ms/epoch - 1ms/step\n",
            "Epoch 65/150\n",
            "216/216 - 0s - loss: 0.2103 - 264ms/epoch - 1ms/step\n",
            "Epoch 66/150\n",
            "216/216 - 0s - loss: 0.2096 - 272ms/epoch - 1ms/step\n",
            "Epoch 67/150\n",
            "216/216 - 0s - loss: 0.2109 - 272ms/epoch - 1ms/step\n",
            "Epoch 68/150\n",
            "216/216 - 0s - loss: 0.2095 - 272ms/epoch - 1ms/step\n",
            "Epoch 69/150\n",
            "216/216 - 0s - loss: 0.2094 - 273ms/epoch - 1ms/step\n",
            "Epoch 70/150\n",
            "216/216 - 0s - loss: 0.2094 - 276ms/epoch - 1ms/step\n",
            "Epoch 71/150\n",
            "216/216 - 0s - loss: 0.2089 - 277ms/epoch - 1ms/step\n",
            "Epoch 72/150\n",
            "216/216 - 0s - loss: 0.2085 - 261ms/epoch - 1ms/step\n",
            "Epoch 73/150\n",
            "216/216 - 0s - loss: 0.2088 - 259ms/epoch - 1ms/step\n",
            "Epoch 74/150\n",
            "216/216 - 0s - loss: 0.2078 - 273ms/epoch - 1ms/step\n",
            "Epoch 75/150\n",
            "216/216 - 0s - loss: 0.2082 - 273ms/epoch - 1ms/step\n",
            "Epoch 76/150\n",
            "216/216 - 0s - loss: 0.2077 - 282ms/epoch - 1ms/step\n",
            "Epoch 77/150\n",
            "216/216 - 0s - loss: 0.2072 - 271ms/epoch - 1ms/step\n",
            "Epoch 78/150\n",
            "216/216 - 0s - loss: 0.2078 - 275ms/epoch - 1ms/step\n",
            "Epoch 79/150\n",
            "216/216 - 0s - loss: 0.2074 - 282ms/epoch - 1ms/step\n",
            "Epoch 80/150\n",
            "216/216 - 0s - loss: 0.2071 - 267ms/epoch - 1ms/step\n",
            "Epoch 81/150\n",
            "216/216 - 0s - loss: 0.2067 - 267ms/epoch - 1ms/step\n",
            "Epoch 82/150\n",
            "216/216 - 0s - loss: 0.2073 - 282ms/epoch - 1ms/step\n",
            "Epoch 83/150\n",
            "216/216 - 0s - loss: 0.2071 - 277ms/epoch - 1ms/step\n",
            "Epoch 84/150\n",
            "216/216 - 0s - loss: 0.2070 - 265ms/epoch - 1ms/step\n",
            "Epoch 85/150\n",
            "216/216 - 0s - loss: 0.2067 - 307ms/epoch - 1ms/step\n",
            "Epoch 86/150\n",
            "216/216 - 0s - loss: 0.2070 - 278ms/epoch - 1ms/step\n",
            "Epoch 87/150\n",
            "216/216 - 0s - loss: 0.2070 - 267ms/epoch - 1ms/step\n",
            "Epoch 88/150\n",
            "216/216 - 0s - loss: 0.2069 - 265ms/epoch - 1ms/step\n",
            "Epoch 89/150\n",
            "216/216 - 0s - loss: 0.2060 - 263ms/epoch - 1ms/step\n",
            "Epoch 90/150\n",
            "216/216 - 0s - loss: 0.2068 - 271ms/epoch - 1ms/step\n",
            "Epoch 91/150\n",
            "216/216 - 0s - loss: 0.2059 - 267ms/epoch - 1ms/step\n",
            "Epoch 92/150\n",
            "216/216 - 0s - loss: 0.2060 - 275ms/epoch - 1ms/step\n",
            "Epoch 93/150\n",
            "216/216 - 0s - loss: 0.2052 - 266ms/epoch - 1ms/step\n",
            "Epoch 94/150\n",
            "216/216 - 0s - loss: 0.2056 - 282ms/epoch - 1ms/step\n",
            "Epoch 95/150\n",
            "216/216 - 0s - loss: 0.2056 - 269ms/epoch - 1ms/step\n",
            "Epoch 96/150\n",
            "216/216 - 0s - loss: 0.2055 - 284ms/epoch - 1ms/step\n",
            "Epoch 97/150\n",
            "216/216 - 0s - loss: 0.2044 - 274ms/epoch - 1ms/step\n",
            "Epoch 98/150\n",
            "216/216 - 0s - loss: 0.2054 - 277ms/epoch - 1ms/step\n",
            "Epoch 99/150\n",
            "216/216 - 0s - loss: 0.2049 - 273ms/epoch - 1ms/step\n",
            "Epoch 100/150\n",
            "216/216 - 0s - loss: 0.2044 - 263ms/epoch - 1ms/step\n",
            "Epoch 101/150\n",
            "216/216 - 0s - loss: 0.2054 - 262ms/epoch - 1ms/step\n",
            "Epoch 102/150\n",
            "216/216 - 0s - loss: 0.2046 - 262ms/epoch - 1ms/step\n",
            "Epoch 103/150\n",
            "216/216 - 0s - loss: 0.2046 - 269ms/epoch - 1ms/step\n",
            "Epoch 104/150\n",
            "216/216 - 0s - loss: 0.2049 - 257ms/epoch - 1ms/step\n",
            "Epoch 105/150\n",
            "216/216 - 0s - loss: 0.2041 - 264ms/epoch - 1ms/step\n",
            "Epoch 106/150\n",
            "216/216 - 0s - loss: 0.2040 - 266ms/epoch - 1ms/step\n",
            "Epoch 107/150\n",
            "216/216 - 0s - loss: 0.2042 - 272ms/epoch - 1ms/step\n",
            "Epoch 108/150\n",
            "216/216 - 0s - loss: 0.2036 - 255ms/epoch - 1ms/step\n",
            "Epoch 109/150\n",
            "216/216 - 0s - loss: 0.2034 - 277ms/epoch - 1ms/step\n",
            "Epoch 110/150\n",
            "216/216 - 0s - loss: 0.2035 - 262ms/epoch - 1ms/step\n",
            "Epoch 111/150\n",
            "216/216 - 0s - loss: 0.2035 - 287ms/epoch - 1ms/step\n",
            "Epoch 112/150\n",
            "216/216 - 0s - loss: 0.2035 - 258ms/epoch - 1ms/step\n",
            "Epoch 113/150\n",
            "216/216 - 0s - loss: 0.2038 - 276ms/epoch - 1ms/step\n",
            "Epoch 114/150\n",
            "216/216 - 0s - loss: 0.2035 - 284ms/epoch - 1ms/step\n",
            "Epoch 115/150\n",
            "216/216 - 1s - loss: 0.2033 - 531ms/epoch - 2ms/step\n",
            "Epoch 116/150\n",
            "216/216 - 0s - loss: 0.2029 - 414ms/epoch - 2ms/step\n",
            "Epoch 117/150\n",
            "216/216 - 0s - loss: 0.2032 - 471ms/epoch - 2ms/step\n",
            "Epoch 118/150\n",
            "216/216 - 1s - loss: 0.2026 - 502ms/epoch - 2ms/step\n",
            "Epoch 119/150\n",
            "216/216 - 1s - loss: 0.2028 - 534ms/epoch - 2ms/step\n",
            "Epoch 120/150\n",
            "216/216 - 1s - loss: 0.2019 - 666ms/epoch - 3ms/step\n",
            "Epoch 121/150\n",
            "216/216 - 1s - loss: 0.2031 - 871ms/epoch - 4ms/step\n",
            "Epoch 122/150\n",
            "216/216 - 1s - loss: 0.2019 - 805ms/epoch - 4ms/step\n",
            "Epoch 123/150\n",
            "216/216 - 1s - loss: 0.2019 - 683ms/epoch - 3ms/step\n",
            "Epoch 124/150\n",
            "216/216 - 1s - loss: 0.2027 - 835ms/epoch - 4ms/step\n",
            "Epoch 125/150\n",
            "216/216 - 1s - loss: 0.2020 - 679ms/epoch - 3ms/step\n",
            "Epoch 126/150\n",
            "216/216 - 1s - loss: 0.2019 - 542ms/epoch - 3ms/step\n",
            "Epoch 127/150\n",
            "216/216 - 1s - loss: 0.2023 - 596ms/epoch - 3ms/step\n",
            "Epoch 128/150\n",
            "216/216 - 1s - loss: 0.2018 - 531ms/epoch - 2ms/step\n",
            "Epoch 129/150\n",
            "216/216 - 0s - loss: 0.2017 - 455ms/epoch - 2ms/step\n",
            "Epoch 130/150\n",
            "216/216 - 0s - loss: 0.2021 - 460ms/epoch - 2ms/step\n",
            "Epoch 131/150\n",
            "216/216 - 1s - loss: 0.2017 - 543ms/epoch - 3ms/step\n",
            "Epoch 132/150\n",
            "216/216 - 0s - loss: 0.2018 - 431ms/epoch - 2ms/step\n",
            "Epoch 133/150\n",
            "216/216 - 0s - loss: 0.2014 - 479ms/epoch - 2ms/step\n",
            "Epoch 134/150\n",
            "216/216 - 0s - loss: 0.2012 - 476ms/epoch - 2ms/step\n",
            "Epoch 135/150\n",
            "216/216 - 0s - loss: 0.2014 - 470ms/epoch - 2ms/step\n",
            "Epoch 136/150\n",
            "216/216 - 0s - loss: 0.2012 - 288ms/epoch - 1ms/step\n",
            "Epoch 137/150\n",
            "216/216 - 0s - loss: 0.2009 - 305ms/epoch - 1ms/step\n",
            "Epoch 138/150\n",
            "216/216 - 0s - loss: 0.2014 - 272ms/epoch - 1ms/step\n",
            "Epoch 139/150\n",
            "216/216 - 0s - loss: 0.2010 - 266ms/epoch - 1ms/step\n",
            "Epoch 140/150\n",
            "216/216 - 0s - loss: 0.2004 - 279ms/epoch - 1ms/step\n",
            "Epoch 141/150\n",
            "216/216 - 0s - loss: 0.2013 - 258ms/epoch - 1ms/step\n",
            "Epoch 142/150\n",
            "216/216 - 0s - loss: 0.2006 - 265ms/epoch - 1ms/step\n",
            "Epoch 143/150\n",
            "216/216 - 0s - loss: 0.2006 - 269ms/epoch - 1ms/step\n",
            "Epoch 144/150\n",
            "216/216 - 0s - loss: 0.2008 - 270ms/epoch - 1ms/step\n",
            "Epoch 145/150\n",
            "216/216 - 0s - loss: 0.2003 - 265ms/epoch - 1ms/step\n",
            "Epoch 146/150\n",
            "216/216 - 0s - loss: 0.2008 - 276ms/epoch - 1ms/step\n",
            "Epoch 147/150\n",
            "216/216 - 0s - loss: 0.2002 - 265ms/epoch - 1ms/step\n",
            "Epoch 148/150\n",
            "216/216 - 0s - loss: 0.2008 - 267ms/epoch - 1ms/step\n",
            "Epoch 149/150\n",
            "216/216 - 0s - loss: 0.2008 - 275ms/epoch - 1ms/step\n",
            "Epoch 150/150\n",
            "216/216 - 0s - loss: 0.2005 - 274ms/epoch - 1ms/step\n",
            "MAE: 0.644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import argmax\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "buQt5Yiwkk1l",
        "outputId": "9afe723d-32ed-4ab5-fb64-50b286899edb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d0673c1d9914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise TypeError(\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             )\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Singleton array 1 cannot be considered a valid collection."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import argmax\n",
        "n_class = len(unique(y))\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(60, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(30, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(n_class, activation='softmax'))\n",
        "# compile the keras model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "precision = precision_score(y_test,yhat)*100\n",
        "recall = recall_score(y_test,yhat)*100\n",
        "f1 = f1_score(y_test,yhat)*100\n",
        "\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1\",f1)\n",
        "\n",
        "print(classification_report(y_test, yhat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If_pqEOAEg3_",
        "outputId": "a4b04ae3-fabc-4773-91e5-925d686f5b7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "226/226 - 1s - loss: 0.7195 - 678ms/epoch - 3ms/step\n",
            "Epoch 2/200\n",
            "226/226 - 0s - loss: 0.6946 - 311ms/epoch - 1ms/step\n",
            "Epoch 3/200\n",
            "226/226 - 0s - loss: 0.6891 - 311ms/epoch - 1ms/step\n",
            "Epoch 4/200\n",
            "226/226 - 0s - loss: 0.6838 - 328ms/epoch - 1ms/step\n",
            "Epoch 5/200\n",
            "226/226 - 0s - loss: 0.6801 - 328ms/epoch - 1ms/step\n",
            "Epoch 6/200\n",
            "226/226 - 0s - loss: 0.6767 - 310ms/epoch - 1ms/step\n",
            "Epoch 7/200\n",
            "226/226 - 0s - loss: 0.6733 - 325ms/epoch - 1ms/step\n",
            "Epoch 8/200\n",
            "226/226 - 0s - loss: 0.6704 - 333ms/epoch - 1ms/step\n",
            "Epoch 9/200\n",
            "226/226 - 0s - loss: 0.6656 - 319ms/epoch - 1ms/step\n",
            "Epoch 10/200\n",
            "226/226 - 0s - loss: 0.6627 - 345ms/epoch - 2ms/step\n",
            "Epoch 11/200\n",
            "226/226 - 0s - loss: 0.6574 - 320ms/epoch - 1ms/step\n",
            "Epoch 12/200\n",
            "226/226 - 0s - loss: 0.6550 - 312ms/epoch - 1ms/step\n",
            "Epoch 13/200\n",
            "226/226 - 0s - loss: 0.6525 - 316ms/epoch - 1ms/step\n",
            "Epoch 14/200\n",
            "226/226 - 0s - loss: 0.6462 - 329ms/epoch - 1ms/step\n",
            "Epoch 15/200\n",
            "226/226 - 0s - loss: 0.6427 - 306ms/epoch - 1ms/step\n",
            "Epoch 16/200\n",
            "226/226 - 0s - loss: 0.6406 - 305ms/epoch - 1ms/step\n",
            "Epoch 17/200\n",
            "226/226 - 0s - loss: 0.6373 - 319ms/epoch - 1ms/step\n",
            "Epoch 18/200\n",
            "226/226 - 0s - loss: 0.6346 - 311ms/epoch - 1ms/step\n",
            "Epoch 19/200\n",
            "226/226 - 0s - loss: 0.6314 - 322ms/epoch - 1ms/step\n",
            "Epoch 20/200\n",
            "226/226 - 0s - loss: 0.6275 - 338ms/epoch - 1ms/step\n",
            "Epoch 21/200\n",
            "226/226 - 0s - loss: 0.6257 - 305ms/epoch - 1ms/step\n",
            "Epoch 22/200\n",
            "226/226 - 0s - loss: 0.6242 - 312ms/epoch - 1ms/step\n",
            "Epoch 23/200\n",
            "226/226 - 0s - loss: 0.6225 - 354ms/epoch - 2ms/step\n",
            "Epoch 24/200\n",
            "226/226 - 0s - loss: 0.6195 - 314ms/epoch - 1ms/step\n",
            "Epoch 25/200\n",
            "226/226 - 0s - loss: 0.6175 - 315ms/epoch - 1ms/step\n",
            "Epoch 26/200\n",
            "226/226 - 0s - loss: 0.6161 - 323ms/epoch - 1ms/step\n",
            "Epoch 27/200\n",
            "226/226 - 0s - loss: 0.6136 - 312ms/epoch - 1ms/step\n",
            "Epoch 28/200\n",
            "226/226 - 0s - loss: 0.6134 - 308ms/epoch - 1ms/step\n",
            "Epoch 29/200\n",
            "226/226 - 0s - loss: 0.6111 - 317ms/epoch - 1ms/step\n",
            "Epoch 30/200\n",
            "226/226 - 0s - loss: 0.6078 - 314ms/epoch - 1ms/step\n",
            "Epoch 31/200\n",
            "226/226 - 0s - loss: 0.6077 - 309ms/epoch - 1ms/step\n",
            "Epoch 32/200\n",
            "226/226 - 0s - loss: 0.6066 - 313ms/epoch - 1ms/step\n",
            "Epoch 33/200\n",
            "226/226 - 0s - loss: 0.6050 - 330ms/epoch - 1ms/step\n",
            "Epoch 34/200\n",
            "226/226 - 0s - loss: 0.6033 - 320ms/epoch - 1ms/step\n",
            "Epoch 35/200\n",
            "226/226 - 0s - loss: 0.6007 - 327ms/epoch - 1ms/step\n",
            "Epoch 36/200\n",
            "226/226 - 0s - loss: 0.6005 - 317ms/epoch - 1ms/step\n",
            "Epoch 37/200\n",
            "226/226 - 0s - loss: 0.6011 - 312ms/epoch - 1ms/step\n",
            "Epoch 38/200\n",
            "226/226 - 0s - loss: 0.5976 - 316ms/epoch - 1ms/step\n",
            "Epoch 39/200\n",
            "226/226 - 0s - loss: 0.5962 - 311ms/epoch - 1ms/step\n",
            "Epoch 40/200\n",
            "226/226 - 0s - loss: 0.5972 - 331ms/epoch - 1ms/step\n",
            "Epoch 41/200\n",
            "226/226 - 0s - loss: 0.5948 - 322ms/epoch - 1ms/step\n",
            "Epoch 42/200\n",
            "226/226 - 0s - loss: 0.5943 - 324ms/epoch - 1ms/step\n",
            "Epoch 43/200\n",
            "226/226 - 0s - loss: 0.5920 - 307ms/epoch - 1ms/step\n",
            "Epoch 44/200\n",
            "226/226 - 0s - loss: 0.5937 - 315ms/epoch - 1ms/step\n",
            "Epoch 45/200\n",
            "226/226 - 0s - loss: 0.5924 - 327ms/epoch - 1ms/step\n",
            "Epoch 46/200\n",
            "226/226 - 0s - loss: 0.5896 - 313ms/epoch - 1ms/step\n",
            "Epoch 47/200\n",
            "226/226 - 0s - loss: 0.5917 - 310ms/epoch - 1ms/step\n",
            "Epoch 48/200\n",
            "226/226 - 0s - loss: 0.5875 - 321ms/epoch - 1ms/step\n",
            "Epoch 49/200\n",
            "226/226 - 0s - loss: 0.5893 - 315ms/epoch - 1ms/step\n",
            "Epoch 50/200\n",
            "226/226 - 0s - loss: 0.5887 - 321ms/epoch - 1ms/step\n",
            "Epoch 51/200\n",
            "226/226 - 0s - loss: 0.5868 - 323ms/epoch - 1ms/step\n",
            "Epoch 52/200\n",
            "226/226 - 0s - loss: 0.5856 - 331ms/epoch - 1ms/step\n",
            "Epoch 53/200\n",
            "226/226 - 0s - loss: 0.5874 - 309ms/epoch - 1ms/step\n",
            "Epoch 54/200\n",
            "226/226 - 0s - loss: 0.5851 - 316ms/epoch - 1ms/step\n",
            "Epoch 55/200\n",
            "226/226 - 0s - loss: 0.5832 - 318ms/epoch - 1ms/step\n",
            "Epoch 56/200\n",
            "226/226 - 0s - loss: 0.5839 - 319ms/epoch - 1ms/step\n",
            "Epoch 57/200\n",
            "226/226 - 0s - loss: 0.5822 - 323ms/epoch - 1ms/step\n",
            "Epoch 58/200\n",
            "226/226 - 0s - loss: 0.5829 - 324ms/epoch - 1ms/step\n",
            "Epoch 59/200\n",
            "226/226 - 0s - loss: 0.5828 - 320ms/epoch - 1ms/step\n",
            "Epoch 60/200\n",
            "226/226 - 0s - loss: 0.5815 - 316ms/epoch - 1ms/step\n",
            "Epoch 61/200\n",
            "226/226 - 0s - loss: 0.5815 - 327ms/epoch - 1ms/step\n",
            "Epoch 62/200\n",
            "226/226 - 0s - loss: 0.5810 - 306ms/epoch - 1ms/step\n",
            "Epoch 63/200\n",
            "226/226 - 0s - loss: 0.5826 - 311ms/epoch - 1ms/step\n",
            "Epoch 64/200\n",
            "226/226 - 0s - loss: 0.5773 - 324ms/epoch - 1ms/step\n",
            "Epoch 65/200\n",
            "226/226 - 0s - loss: 0.5786 - 299ms/epoch - 1ms/step\n",
            "Epoch 66/200\n",
            "226/226 - 0s - loss: 0.5798 - 309ms/epoch - 1ms/step\n",
            "Epoch 67/200\n",
            "226/226 - 0s - loss: 0.5808 - 411ms/epoch - 2ms/step\n",
            "Epoch 68/200\n",
            "226/226 - 0s - loss: 0.5780 - 488ms/epoch - 2ms/step\n",
            "Epoch 69/200\n",
            "226/226 - 1s - loss: 0.5784 - 502ms/epoch - 2ms/step\n",
            "Epoch 70/200\n",
            "226/226 - 0s - loss: 0.5767 - 470ms/epoch - 2ms/step\n",
            "Epoch 71/200\n",
            "226/226 - 0s - loss: 0.5772 - 487ms/epoch - 2ms/step\n",
            "Epoch 72/200\n",
            "226/226 - 1s - loss: 0.5763 - 526ms/epoch - 2ms/step\n",
            "Epoch 73/200\n",
            "226/226 - 1s - loss: 0.5759 - 510ms/epoch - 2ms/step\n",
            "Epoch 74/200\n",
            "226/226 - 1s - loss: 0.5771 - 518ms/epoch - 2ms/step\n",
            "Epoch 75/200\n",
            "226/226 - 0s - loss: 0.5741 - 313ms/epoch - 1ms/step\n",
            "Epoch 76/200\n",
            "226/226 - 0s - loss: 0.5748 - 324ms/epoch - 1ms/step\n",
            "Epoch 77/200\n",
            "226/226 - 0s - loss: 0.5758 - 325ms/epoch - 1ms/step\n",
            "Epoch 78/200\n",
            "226/226 - 0s - loss: 0.5740 - 314ms/epoch - 1ms/step\n",
            "Epoch 79/200\n",
            "226/226 - 0s - loss: 0.5734 - 325ms/epoch - 1ms/step\n",
            "Epoch 80/200\n",
            "226/226 - 0s - loss: 0.5739 - 319ms/epoch - 1ms/step\n",
            "Epoch 81/200\n",
            "226/226 - 0s - loss: 0.5729 - 312ms/epoch - 1ms/step\n",
            "Epoch 82/200\n",
            "226/226 - 0s - loss: 0.5718 - 314ms/epoch - 1ms/step\n",
            "Epoch 83/200\n",
            "226/226 - 0s - loss: 0.5731 - 320ms/epoch - 1ms/step\n",
            "Epoch 84/200\n",
            "226/226 - 0s - loss: 0.5713 - 315ms/epoch - 1ms/step\n",
            "Epoch 85/200\n",
            "226/226 - 0s - loss: 0.5711 - 317ms/epoch - 1ms/step\n",
            "Epoch 86/200\n",
            "226/226 - 0s - loss: 0.5709 - 317ms/epoch - 1ms/step\n",
            "Epoch 87/200\n",
            "226/226 - 0s - loss: 0.5704 - 333ms/epoch - 1ms/step\n",
            "Epoch 88/200\n",
            "226/226 - 0s - loss: 0.5736 - 332ms/epoch - 1ms/step\n",
            "Epoch 89/200\n",
            "226/226 - 0s - loss: 0.5715 - 320ms/epoch - 1ms/step\n",
            "Epoch 90/200\n",
            "226/226 - 0s - loss: 0.5693 - 310ms/epoch - 1ms/step\n",
            "Epoch 91/200\n",
            "226/226 - 0s - loss: 0.5679 - 308ms/epoch - 1ms/step\n",
            "Epoch 92/200\n",
            "226/226 - 0s - loss: 0.5700 - 316ms/epoch - 1ms/step\n",
            "Epoch 93/200\n",
            "226/226 - 0s - loss: 0.5679 - 315ms/epoch - 1ms/step\n",
            "Epoch 94/200\n",
            "226/226 - 0s - loss: 0.5704 - 321ms/epoch - 1ms/step\n",
            "Epoch 95/200\n",
            "226/226 - 0s - loss: 0.5672 - 326ms/epoch - 1ms/step\n",
            "Epoch 96/200\n",
            "226/226 - 0s - loss: 0.5678 - 322ms/epoch - 1ms/step\n",
            "Epoch 97/200\n",
            "226/226 - 0s - loss: 0.5674 - 307ms/epoch - 1ms/step\n",
            "Epoch 98/200\n",
            "226/226 - 0s - loss: 0.5662 - 318ms/epoch - 1ms/step\n",
            "Epoch 99/200\n",
            "226/226 - 0s - loss: 0.5671 - 304ms/epoch - 1ms/step\n",
            "Epoch 100/200\n",
            "226/226 - 0s - loss: 0.5677 - 314ms/epoch - 1ms/step\n",
            "Epoch 101/200\n",
            "226/226 - 0s - loss: 0.5657 - 323ms/epoch - 1ms/step\n",
            "Epoch 102/200\n",
            "226/226 - 0s - loss: 0.5670 - 327ms/epoch - 1ms/step\n",
            "Epoch 103/200\n",
            "226/226 - 0s - loss: 0.5679 - 309ms/epoch - 1ms/step\n",
            "Epoch 104/200\n",
            "226/226 - 0s - loss: 0.5649 - 310ms/epoch - 1ms/step\n",
            "Epoch 105/200\n",
            "226/226 - 0s - loss: 0.5668 - 322ms/epoch - 1ms/step\n",
            "Epoch 106/200\n",
            "226/226 - 0s - loss: 0.5642 - 327ms/epoch - 1ms/step\n",
            "Epoch 107/200\n",
            "226/226 - 0s - loss: 0.5634 - 316ms/epoch - 1ms/step\n",
            "Epoch 108/200\n",
            "226/226 - 0s - loss: 0.5639 - 321ms/epoch - 1ms/step\n",
            "Epoch 109/200\n",
            "226/226 - 0s - loss: 0.5648 - 331ms/epoch - 1ms/step\n",
            "Epoch 110/200\n",
            "226/226 - 0s - loss: 0.5659 - 323ms/epoch - 1ms/step\n",
            "Epoch 111/200\n",
            "226/226 - 0s - loss: 0.5630 - 324ms/epoch - 1ms/step\n",
            "Epoch 112/200\n",
            "226/226 - 0s - loss: 0.5670 - 310ms/epoch - 1ms/step\n",
            "Epoch 113/200\n",
            "226/226 - 0s - loss: 0.5637 - 305ms/epoch - 1ms/step\n",
            "Epoch 114/200\n",
            "226/226 - 0s - loss: 0.5622 - 319ms/epoch - 1ms/step\n",
            "Epoch 115/200\n",
            "226/226 - 0s - loss: 0.5646 - 332ms/epoch - 1ms/step\n",
            "Epoch 116/200\n",
            "226/226 - 0s - loss: 0.5631 - 314ms/epoch - 1ms/step\n",
            "Epoch 117/200\n",
            "226/226 - 0s - loss: 0.5609 - 312ms/epoch - 1ms/step\n",
            "Epoch 118/200\n",
            "226/226 - 0s - loss: 0.5633 - 320ms/epoch - 1ms/step\n",
            "Epoch 119/200\n",
            "226/226 - 0s - loss: 0.5626 - 298ms/epoch - 1ms/step\n",
            "Epoch 120/200\n",
            "226/226 - 0s - loss: 0.5612 - 312ms/epoch - 1ms/step\n",
            "Epoch 121/200\n",
            "226/226 - 0s - loss: 0.5617 - 337ms/epoch - 1ms/step\n",
            "Epoch 122/200\n",
            "226/226 - 0s - loss: 0.5661 - 345ms/epoch - 2ms/step\n",
            "Epoch 123/200\n",
            "226/226 - 0s - loss: 0.5606 - 311ms/epoch - 1ms/step\n",
            "Epoch 124/200\n",
            "226/226 - 0s - loss: 0.5619 - 331ms/epoch - 1ms/step\n",
            "Epoch 125/200\n",
            "226/226 - 0s - loss: 0.5611 - 318ms/epoch - 1ms/step\n",
            "Epoch 126/200\n",
            "226/226 - 0s - loss: 0.5594 - 309ms/epoch - 1ms/step\n",
            "Epoch 127/200\n",
            "226/226 - 0s - loss: 0.5604 - 324ms/epoch - 1ms/step\n",
            "Epoch 128/200\n",
            "226/226 - 0s - loss: 0.5593 - 336ms/epoch - 1ms/step\n",
            "Epoch 129/200\n",
            "226/226 - 0s - loss: 0.5603 - 326ms/epoch - 1ms/step\n",
            "Epoch 130/200\n",
            "226/226 - 0s - loss: 0.5598 - 335ms/epoch - 1ms/step\n",
            "Epoch 131/200\n",
            "226/226 - 0s - loss: 0.5601 - 339ms/epoch - 1ms/step\n",
            "Epoch 132/200\n",
            "226/226 - 0s - loss: 0.5603 - 313ms/epoch - 1ms/step\n",
            "Epoch 133/200\n",
            "226/226 - 0s - loss: 0.5587 - 341ms/epoch - 2ms/step\n",
            "Epoch 134/200\n",
            "226/226 - 0s - loss: 0.5597 - 350ms/epoch - 2ms/step\n",
            "Epoch 135/200\n",
            "226/226 - 0s - loss: 0.5588 - 313ms/epoch - 1ms/step\n",
            "Epoch 136/200\n",
            "226/226 - 0s - loss: 0.5602 - 321ms/epoch - 1ms/step\n",
            "Epoch 137/200\n",
            "226/226 - 0s - loss: 0.5609 - 316ms/epoch - 1ms/step\n",
            "Epoch 138/200\n",
            "226/226 - 0s - loss: 0.5589 - 319ms/epoch - 1ms/step\n",
            "Epoch 139/200\n",
            "226/226 - 0s - loss: 0.5572 - 333ms/epoch - 1ms/step\n",
            "Epoch 140/200\n",
            "226/226 - 0s - loss: 0.5566 - 317ms/epoch - 1ms/step\n",
            "Epoch 141/200\n",
            "226/226 - 0s - loss: 0.5576 - 313ms/epoch - 1ms/step\n",
            "Epoch 142/200\n",
            "226/226 - 0s - loss: 0.5575 - 324ms/epoch - 1ms/step\n",
            "Epoch 143/200\n",
            "226/226 - 0s - loss: 0.5578 - 312ms/epoch - 1ms/step\n",
            "Epoch 144/200\n",
            "226/226 - 0s - loss: 0.5568 - 326ms/epoch - 1ms/step\n",
            "Epoch 145/200\n",
            "226/226 - 0s - loss: 0.5571 - 315ms/epoch - 1ms/step\n",
            "Epoch 146/200\n",
            "226/226 - 0s - loss: 0.5559 - 323ms/epoch - 1ms/step\n",
            "Epoch 147/200\n",
            "226/226 - 0s - loss: 0.5585 - 318ms/epoch - 1ms/step\n",
            "Epoch 148/200\n",
            "226/226 - 0s - loss: 0.5550 - 327ms/epoch - 1ms/step\n",
            "Epoch 149/200\n",
            "226/226 - 0s - loss: 0.5537 - 335ms/epoch - 1ms/step\n",
            "Epoch 150/200\n",
            "226/226 - 0s - loss: 0.5562 - 326ms/epoch - 1ms/step\n",
            "Epoch 151/200\n",
            "226/226 - 0s - loss: 0.5588 - 307ms/epoch - 1ms/step\n",
            "Epoch 152/200\n",
            "226/226 - 0s - loss: 0.5571 - 329ms/epoch - 1ms/step\n",
            "Epoch 153/200\n",
            "226/226 - 0s - loss: 0.5540 - 328ms/epoch - 1ms/step\n",
            "Epoch 154/200\n",
            "226/226 - 0s - loss: 0.5557 - 308ms/epoch - 1ms/step\n",
            "Epoch 155/200\n",
            "226/226 - 0s - loss: 0.5551 - 344ms/epoch - 2ms/step\n",
            "Epoch 156/200\n",
            "226/226 - 0s - loss: 0.5546 - 323ms/epoch - 1ms/step\n",
            "Epoch 157/200\n",
            "226/226 - 0s - loss: 0.5544 - 316ms/epoch - 1ms/step\n",
            "Epoch 158/200\n",
            "226/226 - 0s - loss: 0.5545 - 328ms/epoch - 1ms/step\n",
            "Epoch 159/200\n",
            "226/226 - 0s - loss: 0.5547 - 314ms/epoch - 1ms/step\n",
            "Epoch 160/200\n",
            "226/226 - 0s - loss: 0.5556 - 315ms/epoch - 1ms/step\n",
            "Epoch 161/200\n",
            "226/226 - 0s - loss: 0.5548 - 324ms/epoch - 1ms/step\n",
            "Epoch 162/200\n",
            "226/226 - 0s - loss: 0.5558 - 319ms/epoch - 1ms/step\n",
            "Epoch 163/200\n",
            "226/226 - 0s - loss: 0.5549 - 319ms/epoch - 1ms/step\n",
            "Epoch 164/200\n",
            "226/226 - 0s - loss: 0.5538 - 331ms/epoch - 1ms/step\n",
            "Epoch 165/200\n",
            "226/226 - 0s - loss: 0.5541 - 324ms/epoch - 1ms/step\n",
            "Epoch 166/200\n",
            "226/226 - 0s - loss: 0.5542 - 315ms/epoch - 1ms/step\n",
            "Epoch 167/200\n",
            "226/226 - 0s - loss: 0.5525 - 340ms/epoch - 2ms/step\n",
            "Epoch 168/200\n",
            "226/226 - 0s - loss: 0.5543 - 318ms/epoch - 1ms/step\n",
            "Epoch 169/200\n",
            "226/226 - 0s - loss: 0.5587 - 313ms/epoch - 1ms/step\n",
            "Epoch 170/200\n",
            "226/226 - 0s - loss: 0.5530 - 326ms/epoch - 1ms/step\n",
            "Epoch 171/200\n",
            "226/226 - 0s - loss: 0.5506 - 327ms/epoch - 1ms/step\n",
            "Epoch 172/200\n",
            "226/226 - 0s - loss: 0.5529 - 316ms/epoch - 1ms/step\n",
            "Epoch 173/200\n",
            "226/226 - 0s - loss: 0.5515 - 318ms/epoch - 1ms/step\n",
            "Epoch 174/200\n",
            "226/226 - 0s - loss: 0.5527 - 330ms/epoch - 1ms/step\n",
            "Epoch 175/200\n",
            "226/226 - 0s - loss: 0.5509 - 322ms/epoch - 1ms/step\n",
            "Epoch 176/200\n",
            "226/226 - 0s - loss: 0.5514 - 327ms/epoch - 1ms/step\n",
            "Epoch 177/200\n",
            "226/226 - 0s - loss: 0.5559 - 335ms/epoch - 1ms/step\n",
            "Epoch 178/200\n",
            "226/226 - 0s - loss: 0.5538 - 332ms/epoch - 1ms/step\n",
            "Epoch 179/200\n",
            "226/226 - 0s - loss: 0.5506 - 328ms/epoch - 1ms/step\n",
            "Epoch 180/200\n",
            "226/226 - 0s - loss: 0.5517 - 337ms/epoch - 1ms/step\n",
            "Epoch 181/200\n",
            "226/226 - 0s - loss: 0.5518 - 306ms/epoch - 1ms/step\n",
            "Epoch 182/200\n",
            "226/226 - 0s - loss: 0.5516 - 332ms/epoch - 1ms/step\n",
            "Epoch 183/200\n",
            "226/226 - 0s - loss: 0.5496 - 333ms/epoch - 1ms/step\n",
            "Epoch 184/200\n",
            "226/226 - 0s - loss: 0.5506 - 320ms/epoch - 1ms/step\n",
            "Epoch 185/200\n",
            "226/226 - 0s - loss: 0.5491 - 338ms/epoch - 1ms/step\n",
            "Epoch 186/200\n",
            "226/226 - 0s - loss: 0.5503 - 339ms/epoch - 1ms/step\n",
            "Epoch 187/200\n",
            "226/226 - 0s - loss: 0.5501 - 330ms/epoch - 1ms/step\n",
            "Epoch 188/200\n",
            "226/226 - 0s - loss: 0.5494 - 327ms/epoch - 1ms/step\n",
            "Epoch 189/200\n",
            "226/226 - 0s - loss: 0.5514 - 326ms/epoch - 1ms/step\n",
            "Epoch 190/200\n",
            "226/226 - 0s - loss: 0.5504 - 314ms/epoch - 1ms/step\n",
            "Epoch 191/200\n",
            "226/226 - 0s - loss: 0.5500 - 313ms/epoch - 1ms/step\n",
            "Epoch 192/200\n",
            "226/226 - 0s - loss: 0.5481 - 330ms/epoch - 1ms/step\n",
            "Epoch 193/200\n",
            "226/226 - 0s - loss: 0.5510 - 318ms/epoch - 1ms/step\n",
            "Epoch 194/200\n",
            "226/226 - 0s - loss: 0.5495 - 322ms/epoch - 1ms/step\n",
            "Epoch 195/200\n",
            "226/226 - 0s - loss: 0.5497 - 337ms/epoch - 1ms/step\n",
            "Epoch 196/200\n",
            "226/226 - 0s - loss: 0.5489 - 308ms/epoch - 1ms/step\n",
            "Epoch 197/200\n",
            "226/226 - 0s - loss: 0.5480 - 322ms/epoch - 1ms/step\n",
            "Epoch 198/200\n",
            "226/226 - 0s - loss: 0.5495 - 325ms/epoch - 1ms/step\n",
            "Epoch 199/200\n",
            "226/226 - 0s - loss: 0.5509 - 312ms/epoch - 1ms/step\n",
            "Epoch 200/200\n",
            "226/226 - 0s - loss: 0.5489 - 318ms/epoch - 1ms/step\n",
            "Accuracy: 0.248\n",
            "precision 22.42302543507363\n",
            "recall 22.37808951235805\n",
            "f1 22.400534938147775\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.27      0.27      0.27      1590\n",
            "         1.0       0.22      0.22      0.22      1497\n",
            "\n",
            "    accuracy                           0.25      3087\n",
            "   macro avg       0.25      0.25      0.25      3087\n",
            "weighted avg       0.25      0.25      0.25      3087\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "# Making the Neural Network Classifier\n",
        "NN = MLPClassifier()\n",
        "\n",
        "# Training the model on the training data and labels\n",
        "NN.fit(X_train, y_train)\n",
        "\n",
        "# Testing the model i.e. predicting the labels of the test data.\n",
        "y_pred = NN.predict(X_test)\n",
        "\n",
        "# Evaluating the results of the model\n",
        "accuracy = accuracy_score(y_test,y_pred)*100\n",
        "confusion_mat = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "# Step 8\n",
        "# Printing the Results\n",
        "print(\"Accuracy for Neural Network is:\",accuracy)\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_mat)\n",
        "\n",
        "precision = precision_score(y_test,y_pred)*100\n",
        "recall = recall_score(y_test,y_pred)*100\n",
        "f1 = f1_score(y_test,y_pred)*100\n",
        "\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1\",f1)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-LJCGRmE0NX",
        "outputId": "113782a8-3d5b-4722-9c7d-bc6de7fc82db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Neural Network is: 26.336248785228378\n",
            "Confusion Matrix\n",
            "[[ 444 1146]\n",
            " [1128  369]]\n",
            "precision 24.356435643564357\n",
            "recall 24.649298597194388\n",
            "f1 24.501992031872508\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.28      0.28      0.28      1590\n",
            "         1.0       0.24      0.25      0.25      1497\n",
            "\n",
            "    accuracy                           0.26      3087\n",
            "   macro avg       0.26      0.26      0.26      3087\n",
            "weighted avg       0.26      0.26      0.26      3087\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsAmwn7AlWZn",
        "outputId": "0af01404-7ab4-46dc-ff96-ba8523e9e877"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision 24.356435643564357\n",
            "recall 24.649298597194388\n",
            "f1 24.501992031872508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "scaled = scaler.fit_transform(X)\n",
        "print(scaled)\n",
        "X = pd.DataFrame(scaled)\n",
        "print(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n"
      ],
      "metadata": {
        "id": "kAuHPu_1mEOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e52b6f-c619-4841-cb14-26c1973304a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.81737572  1.22107845  0.71552538 ...  1.56125769  0.162825\n",
            "  -1.56472826]\n",
            " [ 0.87798472 -0.28032963  1.65902253 ...  0.96415241  1.44506014\n",
            "   0.35625972]\n",
            " [ 0.44848422  0.81960462  0.1573605  ...  1.49637975 -0.76907509\n",
            "  -0.61185339]\n",
            " ...\n",
            " [ 1.64734203  1.58640502  0.96075665 ... -1.26329869  1.37203399\n",
            "  -1.28898187]\n",
            " [-1.25164014  0.9172245   1.19359127 ...  1.34223414  1.48038658\n",
            "  -1.00458059]\n",
            " [ 1.15902005  0.21966148  0.64162801 ... -0.82745116 -0.41742504\n",
            "   0.23690818]]\n",
            "            0         1         2         3         4         5         6    \\\n",
            "0     -1.817376  1.221078  0.715525 -1.295860  0.376226 -0.464525 -0.825635   \n",
            "1      0.877985 -0.280330  1.659023 -0.739432  0.584065  0.873201  0.078686   \n",
            "2      0.448484  0.819605  0.157361 -0.543428 -0.770298 -1.724714  0.883040   \n",
            "3     -0.168293  1.272681  0.172472  0.475288 -0.444065 -1.226658 -0.011869   \n",
            "4      0.828649  1.594387  0.861901 -0.136678  0.770365 -0.651056 -0.567475   \n",
            "...         ...       ...       ...       ...       ...       ...       ...   \n",
            "10284 -0.069121  0.585479 -1.028296 -1.430851 -1.409632  0.746020 -1.230662   \n",
            "10285 -0.295844 -1.273245 -0.604538  1.649362  0.542582  1.242389  0.983992   \n",
            "10286  1.647342  1.586405  0.960757 -0.205784  0.416598  1.283645  0.755043   \n",
            "10287 -1.251640  0.917225  1.193591 -0.969797  0.664902  1.011115 -0.745506   \n",
            "10288  1.159020  0.219661  0.641628  0.706237  1.277903 -0.081417 -0.231085   \n",
            "\n",
            "            7         8         9    ...       373       374       375  \\\n",
            "0     -1.505396  0.777750  0.014211  ...  0.980761  1.614898  1.028638   \n",
            "1     -0.892314  1.002356  1.458522  ...  1.636811 -0.988009  0.577954   \n",
            "2      0.835556 -1.202817  1.393140  ... -0.692613 -1.485947  1.900385   \n",
            "3      1.046967  0.343213 -0.418235  ... -1.348943 -0.298635 -0.814887   \n",
            "4     -1.450726  1.479729  1.924379  ...  1.306220  1.509081  1.607848   \n",
            "...         ...       ...       ...  ...       ...       ...       ...   \n",
            "10284  0.934089  0.423639  0.328448  ... -1.265033 -0.987731 -0.807681   \n",
            "10285  0.523293  0.994256  1.392129  ...  0.688755  0.420663 -1.292064   \n",
            "10286  0.982943  1.185656  0.062105  ... -0.564386 -1.870458  1.157572   \n",
            "10287 -0.958283  0.267008 -1.381685  ...  1.142727  1.382730 -0.158452   \n",
            "10288  0.584794 -0.574502  0.101665  ...  0.937419  0.545343  0.986012   \n",
            "\n",
            "            376       377       378       379       380       381       382  \n",
            "0      0.837288 -1.305062  1.275842  1.215062  1.561258  0.162825 -1.564728  \n",
            "1     -0.658468  1.367000  1.060147 -0.140438  0.964152  1.445060  0.356260  \n",
            "2     -0.747899 -0.893414 -0.932264  0.596130  1.496380 -0.769075 -0.611853  \n",
            "3      0.607772 -0.491151  0.255980 -0.145685 -0.187696  0.768246  0.707790  \n",
            "4     -0.185667 -1.354685  1.554518  1.813803 -0.123385  0.266485  0.787816  \n",
            "...         ...       ...       ...       ...       ...       ...       ...  \n",
            "10284  1.419696  0.315412  0.669261  1.399494  0.877416 -1.156817 -0.273576  \n",
            "10285  0.853036  0.011944  0.041409 -0.929889  0.577016  0.466458 -1.517094  \n",
            "10286  1.854667 -0.936675 -0.294736 -0.626307 -1.263299  1.372034 -1.288982  \n",
            "10287 -0.322958 -0.180874  0.753999  0.018446  1.342234  1.480387 -1.004581  \n",
            "10288 -0.645073 -0.122247  0.540405 -0.465847 -0.827451 -0.417425  0.236908  \n",
            "\n",
            "[10289 rows x 383 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "# Making the Neural Network Classifier\n",
        "NN = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n",
        "\n",
        "# Training the model on the training data and labels\n",
        "NN.fit(X_train, y_train)\n",
        "\n",
        "# Testing the model i.e. predicting the labels of the test data.\n",
        "y_pred = NN.predict(X_test)\n",
        "\n",
        "# Evaluating the results of the model\n",
        "accuracy = accuracy_score(y_test,y_pred)*100\n",
        "confusion_mat = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "# Step 8\n",
        "# Printing the Results\n",
        "print(\"Accuracy for Neural Network is:\",accuracy)\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_mat)\n",
        "\n",
        "precision = precision_score(y_test,y_pred)*100\n",
        "recall = recall_score(y_test,y_pred)*100\n",
        "f1 = f1_score(y_test,y_pred)*100\n",
        "\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1\",f1)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuCGHRac-Rik",
        "outputId": "fc63674a-fba3-4f79-b27e-c774a8baa892"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Neural Network is: 26.6489988221437\n",
            "Confusion Matrix\n",
            "[[ 490 1255]\n",
            " [1236  415]]\n",
            "precision 24.850299401197603\n",
            "recall 25.136281041792852\n",
            "f1 24.99247214694369\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.28      0.28      0.28      1745\n",
            "         1.0       0.25      0.25      0.25      1651\n",
            "\n",
            "    accuracy                           0.27      3396\n",
            "   macro avg       0.27      0.27      0.27      3396\n",
            "weighted avg       0.27      0.27      0.27      3396\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "# Making the Neural Network Classifier\n",
        "NN = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
        "\n",
        "# Training the model on the training data and labels\n",
        "NN.fit(X_train, y_train)\n",
        "\n",
        "# Testing the model i.e. predicting the labels of the test data.\n",
        "y_pred = NN.predict(X_test)\n",
        "\n",
        "# Evaluating the results of the model\n",
        "accuracy = accuracy_score(y_test,y_pred)*100\n",
        "confusion_mat = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "# Step 8\n",
        "# Printing the Results\n",
        "print(\"Accuracy for Neural Network is:\",accuracy)\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_mat)\n",
        "\n",
        "precision = precision_score(y_test,y_pred)*100\n",
        "recall = recall_score(y_test,y_pred)*100\n",
        "f1 = f1_score(y_test,y_pred)*100\n",
        "f2 = fbeta_score(y_test,y_pred, beta=2)*100\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1\",f1)\n",
        "print(\"f2\",f2)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm78uGWIBovz",
        "outputId": "50312964-12e6-47b3-9b80-755d32cb5d23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Neural Network is: 31.24263839811543\n",
            "Confusion Matrix\n",
            "[[ 587 1158]\n",
            " [1177  474]]\n",
            "precision 29.044117647058826\n",
            "recall 28.70987280436099\n",
            "f1 28.87602802314955\n",
            "f2 28.77610490529383\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.33      0.34      0.33      1745\n",
            "         1.0       0.29      0.29      0.29      1651\n",
            "\n",
            "    accuracy                           0.31      3396\n",
            "   macro avg       0.31      0.31      0.31      3396\n",
            "weighted avg       0.31      0.31      0.31      3396\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "\n",
        "ann = tf.keras.models.Sequential()\n",
        "#Adding First Hidden Layer\n",
        "ann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n",
        "ann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n",
        "ann.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
        "ann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
        "ann.fit(X_train,y_train,batch_size=32,epochs = 100)\n",
        "yhat = ann.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX70Fn2H-nER",
        "outputId": "c7d59c66-3bcf-423b-a194-08ae74d70a3f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "216/216 [==============================] - 1s 2ms/step - loss: 0.7390 - accuracy: 0.4898\n",
            "Epoch 2/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.7010 - accuracy: 0.5063\n",
            "Epoch 3/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5215\n",
            "Epoch 4/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.5269\n",
            "Epoch 5/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5331\n",
            "Epoch 6/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6856 - accuracy: 0.5437\n",
            "Epoch 7/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5405\n",
            "Epoch 8/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6826 - accuracy: 0.5490\n",
            "Epoch 9/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.5494\n",
            "Epoch 10/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6795 - accuracy: 0.5577\n",
            "Epoch 11/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6779 - accuracy: 0.5591\n",
            "Epoch 12/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6772 - accuracy: 0.5546\n",
            "Epoch 13/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6757 - accuracy: 0.5596\n",
            "Epoch 14/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.5630\n",
            "Epoch 15/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.5691\n",
            "Epoch 16/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.5619\n",
            "Epoch 17/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.5662\n",
            "Epoch 18/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6688 - accuracy: 0.5651\n",
            "Epoch 19/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6682 - accuracy: 0.5678\n",
            "Epoch 20/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6668 - accuracy: 0.5712\n",
            "Epoch 21/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.5722\n",
            "Epoch 22/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.5723\n",
            "Epoch 23/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6631 - accuracy: 0.5755\n",
            "Epoch 24/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6615 - accuracy: 0.5759\n",
            "Epoch 25/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6613 - accuracy: 0.5735\n",
            "Epoch 26/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6600 - accuracy: 0.5749\n",
            "Epoch 27/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6591 - accuracy: 0.5728\n",
            "Epoch 28/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6585 - accuracy: 0.5767\n",
            "Epoch 29/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6581 - accuracy: 0.5836\n",
            "Epoch 30/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6567 - accuracy: 0.5845\n",
            "Epoch 31/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.5788\n",
            "Epoch 32/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.5864\n",
            "Epoch 33/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.5848\n",
            "Epoch 34/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.5855\n",
            "Epoch 35/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.5886\n",
            "Epoch 36/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.5862\n",
            "Epoch 37/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.5812\n",
            "Epoch 38/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6494 - accuracy: 0.5899\n",
            "Epoch 39/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.5913\n",
            "Epoch 40/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6485 - accuracy: 0.5939\n",
            "Epoch 41/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6480 - accuracy: 0.5883\n",
            "Epoch 42/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6470 - accuracy: 0.5920\n",
            "Epoch 43/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.5950\n",
            "Epoch 44/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.5960\n",
            "Epoch 45/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.5951\n",
            "Epoch 46/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.5935\n",
            "Epoch 47/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.5948\n",
            "Epoch 48/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.5964\n",
            "Epoch 49/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.5976\n",
            "Epoch 50/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6427 - accuracy: 0.6024\n",
            "Epoch 51/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.5971\n",
            "Epoch 52/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6414 - accuracy: 0.6013\n",
            "Epoch 53/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6409 - accuracy: 0.5990\n",
            "Epoch 54/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6394 - accuracy: 0.5954\n",
            "Epoch 55/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.6019\n",
            "Epoch 56/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.6024\n",
            "Epoch 57/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6384 - accuracy: 0.6005\n",
            "Epoch 58/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.6019\n",
            "Epoch 59/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6050\n",
            "Epoch 60/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6013\n",
            "Epoch 61/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6363 - accuracy: 0.6063\n",
            "Epoch 62/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.5958\n",
            "Epoch 63/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.6032\n",
            "Epoch 64/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6350 - accuracy: 0.6041\n",
            "Epoch 65/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6358 - accuracy: 0.6021\n",
            "Epoch 66/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6353 - accuracy: 0.6022\n",
            "Epoch 67/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6344 - accuracy: 0.6080\n",
            "Epoch 68/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6325 - accuracy: 0.6084\n",
            "Epoch 69/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6344 - accuracy: 0.6010\n",
            "Epoch 70/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.6039\n",
            "Epoch 71/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.6013\n",
            "Epoch 72/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6321 - accuracy: 0.6074\n",
            "Epoch 73/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6332 - accuracy: 0.6055\n",
            "Epoch 74/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6312 - accuracy: 0.6122\n",
            "Epoch 75/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.6071\n",
            "Epoch 76/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6308 - accuracy: 0.6034\n",
            "Epoch 77/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6309 - accuracy: 0.5980\n",
            "Epoch 78/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.6035\n",
            "Epoch 79/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6309 - accuracy: 0.6051\n",
            "Epoch 80/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.6037\n",
            "Epoch 81/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.6077\n",
            "Epoch 82/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6286 - accuracy: 0.6099\n",
            "Epoch 83/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6282 - accuracy: 0.6108\n",
            "Epoch 84/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6289 - accuracy: 0.6018\n",
            "Epoch 85/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6281 - accuracy: 0.6097\n",
            "Epoch 86/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6268 - accuracy: 0.6093\n",
            "Epoch 87/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6267 - accuracy: 0.6070\n",
            "Epoch 88/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.6071\n",
            "Epoch 89/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.6161\n",
            "Epoch 90/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.6086\n",
            "Epoch 91/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.6048\n",
            "Epoch 92/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.6099\n",
            "Epoch 93/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6259 - accuracy: 0.6068\n",
            "Epoch 94/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6245 - accuracy: 0.6092\n",
            "Epoch 95/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6253 - accuracy: 0.6090\n",
            "Epoch 96/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.6073\n",
            "Epoch 97/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6244 - accuracy: 0.6108\n",
            "Epoch 98/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6233 - accuracy: 0.6060\n",
            "Epoch 99/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.6077\n",
            "Epoch 100/100\n",
            "216/216 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.6135\n",
            "Accuracy: 0.514\n",
            "precision 0.0\n",
            "recall 0.0\n",
            "f1 0.0\n",
            "f2 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      1.00      0.68      1745\n",
            "         1.0       0.00      0.00      0.00      1651\n",
            "\n",
            "    accuracy                           0.51      3396\n",
            "   macro avg       0.26      0.50      0.34      3396\n",
            "weighted avg       0.26      0.51      0.35      3396\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "precision = precision_score(y_test,yhat)\n",
        "recall = recall_score(y_test,yhat)\n",
        "f1 = f1_score(y_test,yhat)\n",
        "f2 = fbeta_score(y_test,yhat, beta=2)\n",
        "\n",
        "\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1\",f1)\n",
        "print(\"f2\",f2)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, yhat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yugCva0CDeI",
        "outputId": "bb049ada-0093-4cda-ca94-73509807a075"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision 0.0\n",
            "recall 0.0\n",
            "f1 0.0\n",
            "f2 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      1.00      0.68      1745\n",
            "         1.0       0.00      0.00      0.00      1651\n",
            "\n",
            "    accuracy                           0.51      3396\n",
            "   macro avg       0.26      0.50      0.34      3396\n",
            "weighted avg       0.26      0.51      0.35      3396\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorflow\n",
        "model_4 = tf.keras.Sequential([\n",
        "\n",
        "                               tf.keras.layers.Dense(4, activation = 'relu'), #we may right it \"tf.keras.activations.relu\" too\n",
        "\n",
        "                               tf.keras.layers.Dense(4, activation = 'relu'),\n",
        "\n",
        "                               tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "\n",
        "])\n",
        "\n",
        "model_4.compile( loss= tf.keras.losses.binary_crossentropy,\n",
        "\n",
        "                optimizer = tf.keras.optimizers.Adam(lr = 0.01),\n",
        "\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "model_4.fit(X_train, y_train, epochs = 200, verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k48kHrkdgwi",
        "outputId": "e86a29fa-771e-4005-ce66-1e280b2d7f6b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f526a0954d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model_4.evaluate(X_test, y_test)\n",
        "print(f' Model loss on the test set: {loss}')\n",
        "print(f' Model accuracy on the test set: {100*accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uorDJRMfptc",
        "outputId": "d00e69f6-28da-4b1b-9ada-55c11c9b0c11"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97/97 [==============================] - 1s 2ms/step - loss: 0.6988 - accuracy: 0.5248\n",
            " Model loss on the test set: 0.6987655162811279\n",
            " Model accuracy on the test set: 52.478134632110596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict probabilities for test set\n",
        "yhats = model_4.predict(X_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "# classes_x=np.argmax(yhats,axis=1)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n"
      ],
      "metadata": {
        "id": "zBdDKFXBfyhg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "precision = precision_score(y_test,yhat)*100\n",
        "recall = recall_score(y_test,yhat)*100\n",
        "f1 = f1_score(y_test,yhat)*100\n",
        "\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1\",f1)\n",
        "\n",
        "print(classification_report(y_test, yhat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIQ3oAHliNSu",
        "outputId": "658dd873-b980-4a37-9dd0-cdbdfe780689"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.544\n",
            "precision 52.50569476082004\n",
            "recall 61.58984635938543\n",
            "f1 56.68613587457732\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.48      0.52      1590\n",
            "         1.0       0.53      0.62      0.57      1497\n",
            "\n",
            "    accuracy                           0.54      3087\n",
            "   macro avg       0.55      0.55      0.54      3087\n",
            "weighted avg       0.55      0.54      0.54      3087\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7kpMBg7i1zm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}